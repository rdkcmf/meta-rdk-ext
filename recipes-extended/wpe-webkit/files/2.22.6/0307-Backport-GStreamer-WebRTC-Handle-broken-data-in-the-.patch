From 1fdd896cc6874303187a8021ba66624ae83217c5 Mon Sep 17 00:00:00 2001
From: Eugene Mutavchi <Ievgen_Mutavchi@comcast.com>
Date: Tue, 5 Jul 2022 20:09:46 +0000
Subject: [PATCH 2/3] Backport [GStreamer][WebRTC] Handle broken data in the
 decoders

https://bugs.webkit.org/show_bug.cgi?id=200584
---
 .../GStreamerVideoDecoderFactory.cpp          | 140 +++++++++++-------
 1 file changed, 90 insertions(+), 50 deletions(-)

diff --git a/Source/WebCore/platform/mediastream/libwebrtc/GStreamerVideoDecoderFactory.cpp b/Source/WebCore/platform/mediastream/libwebrtc/GStreamerVideoDecoderFactory.cpp
index 1c67f80d4fb7..b306bf59d568 100644
--- a/Source/WebCore/platform/mediastream/libwebrtc/GStreamerVideoDecoderFactory.cpp
+++ b/Source/WebCore/platform/mediastream/libwebrtc/GStreamerVideoDecoderFactory.cpp
@@ -53,6 +53,8 @@ class GStreamerVideoDecoder : public webrtc::VideoDecoder {
 public:
     GStreamerVideoDecoder()
         : m_pictureId(0)
+        , m_requireParse(false)
+        , m_needsKeyframe(true)
         , m_firstBufferPts(GST_CLOCK_TIME_NONE)
         , m_firstBufferDts(GST_CLOCK_TIME_NONE)
     {
@@ -84,37 +86,62 @@ public:
         return false;
     }
 
+    void handleError(GError* error)
+    {
+        if (!g_error_matches(error, GST_STREAM_ERROR, GST_STREAM_ERROR_DECODE))
+            return;
+        GST_INFO_OBJECT(pipeline(), "--> needs keyframe (%s)", error->message);
+        m_needsKeyframe = true;
+    }
+
     int32_t InitDecode(const webrtc::VideoCodec*, int32_t) override
     {
         m_src = makeElement("appsrc");
 
+        GRefPtr<GstCaps> caps = nullptr;
         auto capsfilter = CreateFilter();
         auto decoder = makeElement("decodebin");
 
+        m_pipeline = makeElement("pipeline");
+        connectSimpleBusMessageCallback(m_pipeline.get());
+
+        auto sinkpad = adoptGRef(gst_element_get_static_pad(capsfilter, "sink"));
+        g_signal_connect(decoder, "pad-added", G_CALLBACK(decodebinPadAddedCb), sinkpad.get());
         // Make the decoder output "parsed" frames only and let the main decodebin
         // do the real decoding. This allows us to have optimized decoding/rendering
         // happening in the main pipeline.
-        g_object_set(decoder, "caps", adoptGRef(gst_caps_from_string(Caps())).get(), nullptr);
-        auto sinkpad = gst_element_get_static_pad(capsfilter, "sink");
-        g_signal_connect(decoder, "pad-added", G_CALLBACK(decodebinPadAddedCb), sinkpad);
-
-        m_pipeline = makeElement("pipeline");
-        connectSimpleBusMessageCallback(m_pipeline.get());
+        if (m_requireParse) {
+            caps = gst_caps_new_simple(Caps(), "parsed", G_TYPE_BOOLEAN, TRUE, nullptr);
+            GRefPtr<GstBus> bus = adoptGRef(gst_pipeline_get_bus(GST_PIPELINE(m_pipeline.get())));
+            gst_bus_enable_sync_message_emission(bus.get());
+            g_signal_connect_swapped(bus.get(), "sync-message::warning", G_CALLBACK(+[](GStreamerVideoDecoder* decoder, GstMessage* message) {
+                GUniqueOutPtr<GError> error;
+                gst_message_parse_warning(message, &error.outPtr(), nullptr);
+                decoder->handleError(error.get());
+            }), this);
+            g_signal_connect_swapped(bus.get(), "sync-message::error", G_CALLBACK(+[](GStreamerVideoDecoder* decoder, GstMessage* message) {
+                GUniqueOutPtr<GError> error;
+                gst_message_parse_error(message, &error.outPtr(), nullptr);
+                decoder->handleError(error.get());
+            }), this);
+        } else {
+            /* FIXME - How could we handle missing keyframes case we do not plug parsers ? */
+            caps = gst_caps_new_empty_simple(Caps());
+        }
+        g_object_set(decoder, "caps", caps.get(), nullptr);
 
-        auto sink = makeElement("appsink");
-        gst_app_sink_set_emit_signals(GST_APP_SINK(sink), true);
-        g_signal_connect(sink, "new-sample", G_CALLBACK(newSampleCallbackTramp), this);
-        // This is an encoder, everything should happen as fast as possible and not
+        m_sink = makeElement("appsink");
+        // This is an decoder, everything should happen as fast as possible and not
         // be synced on the clock.
-        g_object_set(sink, "sync", false, nullptr);
+        g_object_set(m_sink, "sync", false, nullptr);
 
-        gst_bin_add_many(GST_BIN(pipeline()), m_src, decoder, capsfilter, sink, nullptr);
+        gst_bin_add_many(GST_BIN(pipeline()), m_src, decoder, capsfilter, m_sink, nullptr);
         if (!gst_element_link(m_src, decoder)) {
             GST_ERROR_OBJECT(pipeline(), "Could not link src to decoder.");
             return WEBRTC_VIDEO_CODEC_ERROR;
         }
 
-        if (!gst_element_link(capsfilter, sink)) {
+        if (!gst_element_link(capsfilter, m_sink)) {
             GST_ERROR_OBJECT(pipeline(), "Could not link capsfilter to sink.");
             return WEBRTC_VIDEO_CODEC_ERROR;
         }
@@ -142,11 +169,13 @@ public:
     int32_t Release() final
     {
         if (m_pipeline.get()) {
+            disconnectSimpleBusMessageCallback(m_pipeline.get());
             GRefPtr<GstBus> bus = adoptGRef(gst_pipeline_get_bus(GST_PIPELINE(m_pipeline.get())));
             gst_bus_set_sync_handler(bus.get(), nullptr, nullptr, nullptr);
 
             gst_element_set_state(m_pipeline.get(), GST_STATE_NULL);
             m_src = nullptr;
+            m_sink = nullptr;
             m_pipeline = nullptr;
         }
 
@@ -158,6 +187,19 @@ public:
         const webrtc::CodecSpecificInfo*,
         int64_t renderTimeMs) override
     {
+        if (m_needsKeyframe) {
+            if (inputImage._frameType != webrtc::kVideoFrameKey) {
+                GST_ERROR("Waiting for keyframe but got a delta unit... asking for keyframe");
+                return WEBRTC_VIDEO_CODEC_ERROR;
+            }
+            if (inputImage._completeFrame) {
+                m_needsKeyframe = false;
+            } else {
+                GST_ERROR("Waiting for keyframe but didn't get full frame, getting out");
+                return WEBRTC_VIDEO_CODEC_ERROR;
+            }
+        }
+
         if (!m_src) {
             GST_ERROR("No source set, can't decode.");
 
@@ -175,22 +217,47 @@ public:
             inputImage._size));
         GST_BUFFER_DTS(buffer.get()) = (static_cast<guint64>(inputImage.Timestamp()) * GST_MSECOND) - m_firstBufferDts;
         GST_BUFFER_PTS(buffer.get()) = (static_cast<guint64>(renderTimeMs) * GST_MSECOND) - m_firstBufferPts;
-        {
-            auto locker = holdLock(m_bufferMapLock);
-            InputTimestamps timestamps = {inputImage.Timestamp(), renderTimeMs};
-            m_dtsPtsMap[GST_BUFFER_PTS(buffer.get())] = timestamps;
-        }
 
         GST_LOG_OBJECT(pipeline(), "%" G_GINT64_FORMAT " Decoding: %" GST_PTR_FORMAT, renderTimeMs, buffer.get());
+        InputTimestamps timestamps = {inputImage.Timestamp(), renderTimeMs};
+        m_dtsPtsMap[GST_BUFFER_PTS(buffer.get())] = timestamps;
+
         auto sample = adoptGRef(gst_sample_new(buffer.get(), GetCapsForFrame(inputImage), nullptr, nullptr));
         switch (gst_app_src_push_sample(GST_APP_SRC(m_src), sample.get())) {
         case GST_FLOW_OK:
-            return WEBRTC_VIDEO_CODEC_OK;
+            break;
         case GST_FLOW_FLUSHING:
             return WEBRTC_VIDEO_CODEC_UNINITIALIZED;
         default:
             return WEBRTC_VIDEO_CODEC_ERROR;
         }
+        return pullSample();
+    }
+
+    int32_t pullSample()
+    {
+        auto sample = gst_app_sink_try_pull_sample(GST_APP_SINK(m_sink), GST_SECOND / 30);
+        if (!sample) {
+            GST_ERROR("Needs more data");
+            return WEBRTC_VIDEO_CODEC_OK;
+        }
+        auto buffer = gst_sample_get_buffer(sample);
+
+        // Make sure that the frame.timestamp == previsouly input_frame._timeStamp
+        // as it is required by the VideoDecoder baseclass.
+        auto timestamps = m_dtsPtsMap[GST_BUFFER_PTS(buffer)];
+        m_dtsPtsMap.erase(GST_BUFFER_PTS(buffer));
+
+        auto frame(LibWebRTCVideoFrameFromGStreamerSample(sample, webrtc::kVideoRotation_0,
+            timestamps.timestamp, timestamps.renderTimeMs));
+
+        GST_BUFFER_DTS(buffer) = GST_CLOCK_TIME_NONE;
+        GST_LOG_OBJECT(pipeline(), "Output decoded frame! %d -> %" GST_PTR_FORMAT,
+            frame->timestamp(), buffer);
+
+        m_imageReadyCb->Decoded(*frame.get(), absl::optional<int32_t>(), absl::optional<uint8_t>());
+
+        return WEBRTC_VIDEO_CODEC_OK;
     }
 
     virtual GstCaps* GetCapsForFrame(const webrtc::EncodedImage& image)
@@ -234,30 +301,6 @@ public:
         return decoders != nullptr;
     }
 
-    GstFlowReturn newSampleCallback(GstElement* sink)
-    {
-        auto sample = gst_app_sink_pull_sample(GST_APP_SINK(sink));
-        auto buffer = gst_sample_get_buffer(sample);
-
-        m_bufferMapLock.lock();
-        // Make sure that the frame.timestamp == previsouly input_frame._timeStamp
-        // as it is required by the VideoDecoder baseclass.
-        auto timestamps = m_dtsPtsMap[GST_BUFFER_PTS(buffer)];
-        m_dtsPtsMap.erase(GST_BUFFER_PTS(buffer));
-        m_bufferMapLock.unlock();
-
-        auto frame(LibWebRTCVideoFrameFromGStreamerSample(sample, webrtc::kVideoRotation_0,
-            timestamps.timestamp, timestamps.renderTimeMs));
-
-        GST_BUFFER_DTS(buffer) = GST_CLOCK_TIME_NONE;
-        GST_LOG_OBJECT(pipeline(), "Output decoded frame! %d -> %" GST_PTR_FORMAT,
-            frame->timestamp(), buffer);
-
-        m_imageReadyCb->Decoded(*frame.get(), absl::optional<int32_t>(), absl::optional<uint8_t>());
-
-        return GST_FLOW_OK;
-    }
-
     virtual const gchar* Caps() = 0;
     virtual webrtc::VideoCodecType CodecType() = 0;
     const char* ImplementationName() const { return "GStreamer"; }
@@ -266,20 +309,17 @@ public:
 protected:
     GRefPtr<GstCaps> m_caps;
     gint m_pictureId;
+    bool m_requireParse = false;
+    bool m_needsKeyframe;
 
 private:
-    static GstFlowReturn newSampleCallbackTramp(GstElement* sink, GStreamerVideoDecoder* enc)
-    {
-        return enc->newSampleCallback(sink);
-    }
-
     GRefPtr<GstElement> m_pipeline;
+    GstElement* m_sink;
     GstElement* m_src;
 
     GstVideoInfo m_info;
     webrtc::DecodedImageCallback* m_imageReadyCb;
 
-    Lock m_bufferMapLock;
     StdMap<GstClockTime, InputTimestamps> m_dtsPtsMap;
     GstClockTime m_firstBufferPts;
     GstClockTime m_firstBufferDts;
@@ -287,7 +327,7 @@ private:
 
 class H264Decoder : public GStreamerVideoDecoder {
 public:
-    H264Decoder() { }
+    H264Decoder() { m_requireParse = true; }
 
     int32_t InitDecode(const webrtc::VideoCodec* codecInfo, int32_t nCores) final
     {
-- 
2.25.1

